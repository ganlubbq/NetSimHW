\documentclass[10pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%
%			AZ' STANDARD NEWCOMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite, url,color} % Citation numbers being automatically sorted and properly "compressed/ranged".
%\usepackage{pgfplots}
\usepackage{graphics,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
 \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally does.

%% Useful packages for creation of two-column and more complex figures
% Compact lists
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyvrb}

\usepackage{listings} % inserisce listati di programmi
\definecolor{commenti}{rgb}{0.13,0.55,0.13}
\definecolor{stringhe}{rgb}{0.63,0.125,0.94}
\lstloadlanguages{Matlab}
\lstset{% general command to set parameter(s)
framexleftmargin=0mm,
frame=single,
keywordstyle = \color{blue},% blue keywords
identifierstyle =, % nothing happens
commentstyle = \color{commenti}, % comments
stringstyle = \ttfamily \color{stringhe}, % typewriter type for strings
showstringspaces = false, % no special string spaces
emph = {for, if, then, else, end},
emphstyle = \color{blue},
firstnumber = 1, % numero della prima linea
numbers =right, %  show number_line
numberstyle = \tiny, % style of number_line
stepnumber = 5, % one number_line after stepnumber
numbersep = 5pt,
language = {Matlab}, % per riconoscere la sintassi matlab
extendedchars = true, % per abilitare caratteri particolari
breaklines = true, % per mandare a capo le righe troppo lunghe
breakautoindent = true, % indenta le righe spezzate
breakindent = 30pt, % indenta le righe di 30pt
basicstyle=\footnotesize\ttfamily
}

%Pseudocode package
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{array}
% http://www.ctan.org/tex-archive/macros/latex/required/tools/
\usepackage{mdwmath}
\usepackage{mdwtab}
%mdwtab.sty	-- A complete ground-up rewrite of LaTeX's `tabular' and  `array' environments.  Has lots of advantages over
%		   the standard version, and over the version in `array.sty'.
% *** SUBFIGURE PACKAGES ***
\usepackage[tight,footnotesize]{subfigure}

\usepackage[top=2cm, bottom=2cm, right=1.6cm,left=1.6cm]{geometry}
\usepackage{indentfirst}

%\usepackage{times}
%\usepackage[active]{srcltx}

\setlength\parindent{0pt}
\linespread{1}

\def\C#1{\mathcal{#1}}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% Package used to keep inherent figures in the same section
\usepackage{placeins}


\begin{document}
\title{Network Analysis and Simulation - Homework 2}
\author{Michele Polese, 1100877}

\maketitle

\section{Exercise 1}
A \textit{Linear Congruential Generator} (LCG) is a pseudorandom number generator, characterized by the parameters $a, c, m, x_0$. Generally the sequence $\{x_n\}$ of random numbers is generated by iterating: $x_n = (ax_{n-1} + c) \text{mod} m$. If $c = 0$ then the LCG is a multiplicative LCG and the maximum period of the sequence is $m-1$ because $x_n = 0$ would be a standpoint for the generator and it is never reached, unless $x_0 = 0$ (but this would be a very bad choice).
Figure~\ref{fig:65} shows the randomness of a U[0,1] sequence generated with a LCG by normalizing the $x_n$ sequence to $m$ in different ways: by comparing with a U[0, 1] generated by MATLAB Mersenne Twister rng, by showing the lac of correlation between samples with the autocorrelation function and lag plots.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{images/hw2_1_65}
  \caption{Figure 6.5 in \cite{leb}}
  \label{fig:65}
\end{figure}

A LCG however must be handled carefully when dealing with parallel streams. In Figure~\ref{fig:67} there are two lag plots at lag 1 which show that the behavior of a LCG depends on the initial seed. If the two seeds depend one on the other or are not randomly chosen, for example with entropy extraction, as in the first plot where $x_0^{\text{LGC}_1} = 1$ and $x_0^{\text{LGC}_2} = 2$, then there's a strong correlation between the two streams (actually up to the wrap around $x_i^{\text{LGC}_2} = 2x_i^{\text{LGC}_1}$). Instead, if the seed of the second stream is the last element of the first sequence and the total number of samples generated doesn't exceed the period of the LCG then the two sequences are uncorrelated.
\begin{figure}
  \centering
  \subfigure{\includegraphics[width=0.7\textwidth]{images/hw2_1_67a}}
  \subfigure{\includegraphics[width=0.7\textwidth]{images/hw2_1_67b}}
  \caption{Figure 6.7 in \cite{leb}}
  \label{fig:67}
\end{figure}

In Figure~\ref{fig:610} there are two distributions generated with rejection sampling. This technique allows to compute a random variable with a certain probability density distribution which is not completely known (i.e. missing normalization factor) by comparing uniform random variables with the expected values.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{images/hw2_1_610}
  \caption{Figure 6.10 in \cite{leb}}
  \label{fig:610}
\end{figure}

\section{Exercise 2}
A Binomial random variable (Bin($n, p$)) can be generated in three different ways. The first is the CDF inversion, which can be computed in an iterative way. Since the CDF of a Bin($n, p$) is $F(r) = \sum_{k = 0}^{r} \frac{n!}{(n-k)!k!} (1-p)^{n-k} p^k$ cannot be inverted in a close form, it is possible to compute it in an iterative way with the following algorithm:

\begin{algorithm}
  \caption{CDF inversion for Bin($n, p$)}\label{cdfinvbin}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $X = 0, pr = (1-p)^n, F = pr, i = 0$
    \While {$U >= F$}
    \State $X = X + 1$
    \State $pr = \frac{n-i}{i+1} \frac{p}{1-p} pr$
    \State $F = F + pr$
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The second algorithm exploits the nature of the binomial distribution, which represents the number of success in $n$ Bernoulli trials with probability of success $p$. Therefore

\begin{algorithm}
  \caption{Generation of a Bin($n, p$) with $n$ bernoulli trials}\label{ber}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0, i = 1$
    \While {$ i \le n$}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \If {$U <= p$}
    \State $X = X + 1$
    \EndIf
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

A more efficient variant of this method involves the generation of strings of $0$ (unsuccessful Bernoulli trials with $P_{\text{succ}} = p$) followed by a $1$, which is the first successful Bernoulli trial. These strings are distributed according to a geometric random variable $G(p)$. A geometric random variable can be generated with CDF inversion in a closed form, using $G = \floor{\frac{\log(U)}{\log(1-p)}}$ with $U$ a uniform sample in $[0,1]$. Thus
\begin{algorithm}
  \caption{Generation of a Bin($n, p$) with geometric strings of $0$}\label{geo}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $G = \floor{\frac{\log(U)}{\log(1-p)}}$ the length of a string of zeros
    \State Let $i = G + 1$ a string of $G$ zeros and a $1$
    \While {$ i \le n$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $G = \floor{\frac{\log(U)}{\log(1-p)}}$ the length of a string of zeros
    \State $i = i +G+ 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The algorithms are implemented in the attached MATLAB code in order to compare their performances. Note that the average number of iterations for Algorithm~\ref{cdfbininv} has to perform is one more than the value of the random variable it generates, so on average $1 + np$. Algorithm~\ref{ber} instead performs always $n$ iterations. The generation of geometric strings of zeros has a complexity which is proportional to $np$ too, since on average the strings have $\frac{1-p}{p}$ zeros and a 1, thus are long $1/p$: therefore the number of iterations needed to reach $n$ are on average $\frac{n}{1/p} = np$ (the comparisons are $1+np$). \\
The relation between the three methods can be seen in Figures~\ref{fig:bin},~\ref{fig:bin_small}. CDF inversion and geometric method should perform approximately in the same way. Actually when there are many iterations the complex operations that Algorithm~\ref{geo} has to perform in each iteration (a logarithm, a division, an extraction of random number) make it slower that the simple CDF inversion. Figure~\ref{fig:bin}c has on x and y axis the time needed to generate $10^5$ with CDF inversion and geometric strings, respectively (each point represents the time to generate $10^5$ Bin($n,p$) with the same $n$ and $p$). It can be seen that the two methods have a linear dependance, which means that they share the same complexity (as expected) but the time required to execute each iteration differs by a constant. Instead when $np$ is small and the number of iterations is on average lower than 1 then the two methods perform approximately in constant time. This can be seen in Figure~\ref{fig:bin_small} where it is plotted the time required to generate $10^5$ binomial random variables with $n\in [20, 10^4]$ (increased by a step of 20) and $p \in [10^{-9}, 10^{-8}, 10^{-7}, 10^{-6}, 10^{-5}]$ and in Figures .
In this case the geometric strings method has a weak dependence on $p$, probably due to the generation of the geometric random variable. Note also that the CDF inversion method is based on iterations, thus the values it computes are subject to approximation errors. Moreover it must be taken into account the limit of the finite precision of a computer, and since the lowest positive number which can be represented in MATLAB is $\delta = 4.9407e-324$ then for values of $n$ and $p$ such that $(1-p)^n < e$ the CDF inversion cannot be performed. This can happen for $p \approx 1/2$ and $n > 1000$. \\
Another observation is that despite the dependance on $n$ and not on $np$ of the bernoulli strings method there are some cases in which it performs better than the geometric strings method. Indeed if in principle the number of iterations of the second method are less, their complexity is higher, therefore for values of $p$ which are close to 0.5 then the Bernoulli becomes faster.

\begin{figure}[h]
  \centering
  \subfigure[Bernoulli trials vs CDF inversion]{\includegraphics[width=0.45\textwidth]{images/inv_ber}}
  \subfigure[Bernoulli trials vs geometric strings]{\includegraphics[width=0.45\textwidth]{images/geo_ber}}
  \subfigure[Geometric strings vs CDF inversion]{\includegraphics[width=0.45\textwidth]{images/inv_geo}}
  \caption{Comparison between time required to generate $N = 10^5$ binomial rv}
  \label{fig:bin}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.8\textwidth]{images/bin_smallp}
  \caption{Geometric strings vs CDF inversion methods' execution time for $np < 1$}
  \label{fig:bin_small}
\end{figure}


\section{Exercise 3}
A random variable which follows a Poisson distribution with parameter $\lambda$ can be generated in three ways: with CDF inversion (in an iterative fashion) and exploiting the property that link a Poisson distribution with the Poisson Process of intensity $\lambda$. \\
CDF inversion is performed in an iterative way. The CDF of a Poisson process is $F(k) = \sum_{i = 0}^k e^{-\lambda} \frac{\lambda ^i}{i!}$ and it can be written as $F(k+1) = \frac{\lambda}{k+1} F(k) + F(k)$ with $F(0) = e^{-\lambda}$. The following algorithm exploits this property:

\begin{algorithm}
  \caption{CDF inversion for Poisson($\lambda$)}\label{cdfinvpoi}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $X = 0, pr = e^{-\lambda}, F = pr, i = 0$
    \While {$U >= F$}
    \State $X = X + 1$
    \State $pr = \frac{\lambda}{i+1}pr$
    \State $F = F + pr$
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The second algorithm is based on the following fact. In a Poisson process with intensity $\lambda$ the number of events in a time interval $t$ is distributed according to a Poisson distribution with mean $\lambda t$. The time between each event is an exponential with mean $\frac{1}{\lambda}$. The algorithm counts the number of events $X$ in a time interval $t=1$ by generating exponential random variables until they sum up to 1. The exact procedure is described in Algorithm~\ref{exp}. Note that an exponential random variable can be generated by the direct inversion of its CDF as like as the the geometric rv, using the formula $E = \frac{-1}{\lambda} \log(U)$ with $U$ a uniform $U[0,1]$ random variable.

\begin{algorithm}
  \caption{Generation of a Poisson($\lambda$) with exponential interarrival times}\label{exp}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $E = \frac{-1}{\lambda} \log(U)$ the time of next event
    \State Let $i = E$ the time of last event
    \While {$ i \le 1$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $E = \frac{-1}{\lambda} \log(U)$
    \State $i = i + E$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

In the previous the Poisson random variable $X$ is defined as $X = \argmax_n{\sum_{i = 0}^n E_i \le 1}$ with $E_i = = \frac{-1}{\lambda} \log(U_i)$. Therefore $X = \argmax_n{\frac{-1}{\lambda} \sum_{i = 0}^n \log(U_i) \le 1} = \argmax_n{\frac{-1}{\lambda} \log( \Pi_{i = 0}^n U_i ) \le 1}$ and finally $X = \argmin_n \Pi_{i = 0}^n U_i > e^{-\lambda}$. Therefore the third algorithm is described by the following pseudocode.

\begin{algorithm}
  \caption{Generation of a Poisson($\lambda$) with product of uniforms}\label{prod}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $i = U$
    \While {$ i \le e^{-\lambda}$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $i = Ui$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

All three methods have a computational complexity which is proportional to $\lambda$, as it can be seen in Figure~\ref{fig:poi_iter}. However the implementation of the second algorithm is much more expensive, since it requires to compute a logarithm and a division at each iteration. This can be seen in Figures~\ref{fig:} and~\ref{fig:} where the time to generate $10^5$ Poisson random variables with the three algorithms is plotted against the value of $\lambda$. In Figure~\ref{fig:} $\lambda \in [0.001, 10]$ while in Figure~\ref{} $\lambda \in [10, 740]$. For $\lambda > 745.14$ the CDF inversion is unfeasible since the starting $pr(k=0) = e^{-\lambda} < \delta$ with $\delta = 4.9407e-324$ the lowest positive number which can be represented in MATLAB, so it is approximated to 0.

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.8\textwidth]{images/poi_iter}
  \caption{Iterations required to generate $N = 1000$ Poisson random variables with the three methods}
  \label{fig:poi_iter}
\end{figure}

\begin{figure}
  \centering
  \includegraphics

\section{Exercise 4}
Let the first linear congruential generator (LCG1) have $a = 18, m = 101, c = 0$ and the second (LCG2) $a = 2$ and the same $m, c$. They are both full period. Indeed by generating a sequence of $m-1$ samples there are no repeated samples for both of them. Note that a period for a multiplicative LCG ($c = 0$) is $m-1$.\\
Figure ~\ref{fig:lcg} contains the lag plot for lag 1 of these generators. From Figure~\ref{fig:lcg}c it can seen that the samples of LCG2 are strongly correlated because of the bad choices of the LCGs parameters. Actually, up to the wrap around, for any choice of $x_0$, the sample $n+1$ is $a = 2$ times the sample $n$. Moreover, since the number of possible values is small, the randomness of the sequence is limited. The other LCG seems to have samples which are uniformly distributed in a 2d space at lag 1, with rows of points which are equally spaced, but by looking at the analysis in three dimensions in Figure~\ref{fig:lcg}b it can be clearly seen that they are not well distributed and that they fall into hyperplanes, as it always happens for LCG (Masaglia Theorem \cite{masaglia}).

\begin{figure}
  \centering
  \subfigure[LCG1]{\includegraphics[width = 0.45\textwidth]{images/hw2_4_lcg1}}
  \subfigure[LCG1 in 3 dimensions]{\includegraphics[width = 0.45\textwidth]{images/hw2_4_lcg1_3d}}
  \subfigure[LCG2]{\includegraphics[width = 0.45\textwidth]{images/hw2_4_lcg2}}
  \caption{Lag plots for LCG1 and LCG2}
  \label{fig:lcg}
\end{figure}

\section{Exercise 5}
The third LCG under analysis belongs to the family of LCGs with $m = 2^M$, in particular $m = 2^{31}$ and $a = 65539$, which is a prime number ($c = 0$ as usual). If the seed is an odd number (for example $x_0 = 1$) these are the parameters of the rng called RANDU, a random number generator which was designed by IBM in the 1960s \cite{kn}. Apparently, if just 2 dimensions are observed, the numbers at lag 1 are equally distributed in the unit square and there are not hyperplanes structures as shown in Figure~\ref{fig:randu2d}. However if another dimension is taken into account the correlation between subsequent samples is clear, since there are 15 hyperplanes on which the points are distributed, approximately with the same distance one to each other as it can be seen in Figure~\ref{fig:randu3d}. As previously stated, this is a common result in LCG analysis. However Masaglia Theorem states that if n-tuples of subsequent samples of a LCG are considered then there are at most $n!m^{\frac{1}{n}}$ hyperplanes in n-space (and they have $n-1$ dimensionality and are parallel). The more the number of actual hyperplanes is closer to this limit the better the rng is. For n=3 and RNADU rng ($m = 2^{31}$), there should be at most $(3! 2^{31})^{1/3} \approx 2344$ hyperplanes, but the triplets of this LCG are distributed only on 15 of them and this shows the weakness of RANDU in generating samples which are uncorrelated.

\begin{figure}
  \centering
  \subfigure[1000 samples]{\includegraphics[width = 0.3\textwidth]{images/hw2_5_1000}}
  \subfigure[10000 samples]{\includegraphics[width = 0.3\textwidth]{images/hw2_5_10000}}
  \subfigure[20000 samples]{\includegraphics[width = 0.3\textwidth]{images/hw2_5_20000}}
  \caption{Lag plots for RANDU, lag = 1, $x0 = 1$}
  \label{fig:randu2d}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width = 0.9\textwidth]{images/hw2_5_3d}
  \caption{Lag plot for RANDU in three dimension, each point is $x_n, x_{n+1}, x_{n+2}$, 20000 points}
  \label{fig:randu3d}
\end{figure}

\begin{thebibliography}{10}

\bibitem{leb}
Y. Le Boudec, Performance Evaluation of Computer and Communications Systems, EPFL, 2015

\bibitem{pk}
M. Pinsky, S. Karlin, An Introduction to Stochastic Modeling, $4^{th}$ edition, Elsevier, 2011

\bibitem{kn}
D.E. Knuth, The Art of Computer Programming, volume 2: Seminumerical Algorithms, Addison-Wesley, Reading, MA, 2nd edition, 1981.

\bibitem{masaglia}
G. Masaglia, Random numbers fall mainly in the planes, Mathematics Research Laboratory, Boeing Scientific Research Laboratories, 1968


\end{thebibliography}

\end{document}
