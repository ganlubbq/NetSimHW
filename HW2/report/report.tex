\documentclass[10pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%
%			AZ' STANDARD NEWCOMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite, url,color} % Citation numbers being automatically sorted and properly "compressed/ranged".
%\usepackage{pgfplots}
\usepackage{graphics,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
 \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally does.

%% Useful packages for creation of two-column and more complex figures
% Compact lists
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyvrb}

\usepackage{listings} % inserisce listati di programmi
\definecolor{commenti}{rgb}{0.13,0.55,0.13}
\definecolor{stringhe}{rgb}{0.63,0.125,0.94}
\lstloadlanguages{Matlab}
\lstset{% general command to set parameter(s)
framexleftmargin=0mm,
frame=single,
keywordstyle = \color{blue},% blue keywords
identifierstyle =, % nothing happens
commentstyle = \color{commenti}, % comments
stringstyle = \ttfamily \color{stringhe}, % typewriter type for strings
showstringspaces = false, % no special string spaces
emph = {for, if, then, else, end},
emphstyle = \color{blue},
firstnumber = 1, % numero della prima linea
numbers =right, %  show number_line
numberstyle = \tiny, % style of number_line
stepnumber = 5, % one number_line after stepnumber
numbersep = 5pt,
language = {Matlab}, % per riconoscere la sintassi matlab
extendedchars = true, % per abilitare caratteri particolari
breaklines = true, % per mandare a capo le righe troppo lunghe
breakautoindent = true, % indenta le righe spezzate
breakindent = 30pt, % indenta le righe di 30pt
basicstyle=\footnotesize\ttfamily
}

%Pseudocode package
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{array}
% http://www.ctan.org/tex-archive/macros/latex/required/tools/
\usepackage{mdwmath}
\usepackage{mdwtab}
%mdwtab.sty	-- A complete ground-up rewrite of LaTeX's `tabular' and  `array' environments.  Has lots of advantages over
%		   the standard version, and over the version in `array.sty'.
% *** SUBFIGURE PACKAGES ***
\usepackage[tight,footnotesize]{subfigure}

\usepackage[top=2cm, bottom=2cm, right=1.6cm,left=1.6cm]{geometry}
\usepackage{indentfirst}

%\usepackage{times}
%\usepackage[active]{srcltx}

\setlength\parindent{0pt}
\linespread{1}

\def\C#1{\mathcal{#1}}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% Package used to keep inherent figures in the same section
\usepackage{placeins}


\begin{document}
\title{Network Analysis and Simulation - Homework 2}
\author{Michele Polese, 1100877}

\maketitle

\section{Exercise 1}
A \textit{Linear Congruential Generator} (LCG) is a pseudorandom number generator, characterized by the parameters $a, c, m, x_0$. Generally the sequence $\{x_n\}$ of random numbers is generated by iterating: $x_n = (ax_{n-1} + c) \text{mod} m$. If $c = 0$ then the LCG is a multiplicative LCG and the maximum period of the sequence is $m-1$ because $x_n = 0$ would be a standpoint for the generator and it is never reached, unless $x_0 = 0$ (but this would be a very bad choice).
Figure~\ref{fig:65} shows the randomness of a U[0,1] sequence generated with a LCG by normalizing the $x_n$ sequence to $m$ in different ways: by comparing with a U[0, 1] generated by MATLAB Mersenne Twister rng, by showing the lac of correlation between samples with the autocorrelation function and lag plots.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{images/hw2_1_65}
  \caption{Figure 6.5 in \cite{leb}}
  \label{fig:65}
\end{figure}

A LCG however must be handled carefully when dealing with parallel streams. In Figure~\ref{fig:67} there are two lag plots at lag 1 which show that the behavior of a LCG depends on the initial seed. If the two seeds depend one on the other or are not randomly chosen, for example with entropy extraction, as in the first plot where $x_0^{\text{LGC}_1} = 1$ and $x_0^{\text{LGC}_2} = 2$, then there's a strong correlation between the two streams (actually up to the wrap around $x_i^{\text{LGC}_2} = 2x_i^{\text{LGC}_1}$). Instead, if the seed of the second stream is the last element of the first sequence and the total number of samples generated doesn't exceed the period of the LCG then the two sequences are uncorrelated.
\begin{figure}
  \centering
  \subfigure{\includegraphics[width=0.7\textwidth]{images/hw2_1_67a}}
  \subfigure{\includegraphics[width=0.7\textwidth]{images/hw2_1_67b}}
  \caption{Figure 6.7 in \cite{leb}}
  \label{fig:67}
\end{figure}

In Figure~\ref{fig:610} there are two distributions generated with rejection sampling. This technique allows to compute a random variable with a certain probability density distribution which is not completely known (i.e. missing normalization factor) by comparing uniform random variables with the expected values.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{images/hw2_1_610}
  \caption{Figure 6.10 in \cite{leb}}
  \label{fig:610}
\end{figure}

\section{Exercise 2}
A Binomial random variable (Bin($n, p$)) can be generated in three different ways. The first is the CDF inversion, which can be computed in an iterative way. Since the CDF of a Bin($n, p$) is $F(r) = \sum_{k = 0}^{r} \frac{n!}{(n-k)!k!} (1-p)^{n-k} p^k$ cannot be inverted in a close form, it is possible to compute it in an iterative way with the following algorithm:

\begin{algorithm}
  \caption{CDF inversion for Bin($n, p$)}\label{cdfinvbin}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $X = 0, pr = (1-p)^n, F = pr, i = 0$
    \While {$U >= F$}
    \State $X = X + 1$
    \State $pr = \frac{n-i}{i+1} \frac{p}{1-p} pr$
    \State $F = F + pr$
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The second algorithm exploits the nature of the binomial distribution, which represents the number of success in $n$ Bernoulli trials with probability of success $p$. Therefore

\begin{algorithm}
  \caption{Generation of a Bin($n, p$) with $n$ bernoulli trials}\label{ber}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0, i = 1$
    \While {$ i \le n$}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \If {$U <= p$}
    \State $X = X + 1$
    \EndIf
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

A more efficient variant of this method involves the generation of strings of $0$ (unsuccessful Bernoulli trials with $P_{\text{succ}} = p$) followed by a $1$, which is the first successful Bernoulli trial. These strings are distributed according to a geometric random variable $G(p)$. A geometric random variable can be generated with CDF inversion in a closed form, using $G = \floor{\frac{\log(U)}{\log(1-p)}}$ with $U$ a uniform sample in $[0,1]$. Thus
\begin{algorithm}
  \caption{Generation of a Bin($n, p$) with geometric strings of $0$}\label{geo}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $G = \floor{\frac{\log(U)}{\log(1-p)}}$ the length of a string of zeros
    \State Let $i = G + 1$ a string of $G$ zeros and a $1$
    \While {$ i \le n$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $G = \floor{\frac{\log(U)}{\log(1-p)}}$ the length of a string of zeros
    \State $i = i +G+ 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The algorithms are implemented in the attached MATLAB code in order to compare their performances. Note that the average number of iterations for Algorithm~\ref{cdfbininv} has to perform is one more than the value of the random variable it generates, so on average $1 + np$. Algorithm~\ref{ber} instead performs always $n$ iterations. The generation of geometric strings of zeros has a complexity which is proportional to $np$ too, since on average the strings have $\frac{1-p}{p}$ zeros and a 1, thus are long $1/p$: therefore the number of iterations needed to reach $n$ are on average $\frac{n}{1/p} = np$ (the comparisons are $1+np$). \\
The relation between the three methods can be seen in Figures~\ref{fig:bin},~\ref{fig:bin_small}. CDF inversion and geometric method should perform approximately in the same way. Actually when there are many iterations the complex operations that Algorithm~\ref{geo} has to perform in each iteration (a logarithm, a division, an extraction of random number) make it slower that the simple CDF inversion. Figure~\ref{fig:bin}c has on x and y axis the time needed to generate $10^5$ with CDF inversion and geometric strings, respectively (each point represents the time to generate $10^5$ Bin($n,p$) with the same $n$ and $p$). It can be seen that the two methods have a linear dependance, which means that they share the same complexity (as expected) but the time required to execute each iteration differs by a constant. Instead when $np$ is small and the number of iterations is on average lower than 1 then the two methods perform approximately in constant time. This can be seen in Figure~\ref{fig:bin_small} where it is plotted the time required to generate $10^5$ binomial random variables with $n\in [20, 10^4]$ (increased by a step of 20) and $p \in [10^{-9}, 10^{-8}, 10^{-7}, 10^{-6}, 10^{-5}]$ and in Figures .
In this case the geometric strings method has a weak dependence on $p$, probably due to the generation of the geometric random variable. Note also that the CDF inversion method is based on iterations, thus the values it computes are subject to approximation errors. Moreover it must be taken into account the limit of the finite precision of a computer, and since the lowest positive number which can be represented in MATLAB is $\delta = 4.9407e-324$ then for values of $n$ and $p$ such that $(1-p)^n < e$ the CDF inversion cannot be performed. This can happen for $p \approx 1/2$ and $n > 1000$. \\
Another observation is that despite the dependance on $n$ and not on $np$ of the bernoulli strings method there are some cases in which it performs better than the geometric strings method. Indeed if in principle the number of iterations of the second method are less, their complexity is higher, therefore for values of $p$ which are close to 0.5 then the Bernoulli becomes faster.

\begin{figure}[h]
  \centering
  \subfigure[Bernoulli trials vs CDF inversion]{\includegraphics[width=0.45\textwidth]{images/inv_ber}}
  \subfigure[Bernoulli trials vs geometric strings]{\includegraphics[width=0.45\textwidth]{images/geo_ber}}
  \subfigure[Geometric strings vs CDF inversion]{\includegraphics[width=0.45\textwidth]{images/inv_geo}}
  \caption{Comparison between time required to generate $N = 10^5$ binomial rv}
  \label{fig:bin}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.8\textwidth]{images/bin_smallp}
  \caption{Geometric strings vs CDF inversion methods' execution time for $np < 1$}
  \label{fig:bin_small}
\end{figure}


\section{Exercise 3}
A random variable which follows a Poisson distribution with parameter $\lambda$ can be generated in three ways: with CDF inversion (in an iterative fashion) and exploiting the property that link a Poisson distribution with the Poisson Process of intensity $\lambda$. \\
CDF inversion is performed in an iterative way. The CDF of a Poisson process is $F(k) = \sum_{i = 0}^k e^{-\lambda} \frac{\lambda ^i}{i!}$ and it can be written as $F(k+1) = \frac{\lambda}{k+1} F(k) + F(k)$ with $F(0) = e^{-\lambda}$. The following algorithm exploits this property:

\begin{algorithm}
  \caption{CDF inversion for Poisson($\lambda$)}\label{cdfinvpoi}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $X = 0, pr = e^{-\lambda}, F = pr, i = 0$
    \While {$U >= F$}
    \State $X = X + 1$
    \State $pr = \frac{\lambda}{i+1}pr$
    \State $F = F + pr$
    \State $i = i + 1$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The second algorithm is based on the following fact. In a Poisson process with intensity $\lambda$ the number of events in a time interval $t$ is distributed according to a Poisson distribution with mean $\lambda t$. The time between each event is an exponential with mean $\frac{1}{\lambda}$. The algorithm counts the number of events $X$ in a time interval $t=1$ by generating exponential random variables until they sum up to 1. The exact procedure is described in Algorithm~\ref{exp}. Note that an exponential random variable can be generated by the direct inversion of its CDF as like as the the geometric rv, using the formula $E = \frac{-1}{\lambda} \log(U)$ with $U$ a uniform $U[0,1]$ random variable.

\begin{algorithm}
  \caption{Generation of a Poisson($\lambda$) with exponential interarrival times}\label{exp}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $E = \frac{-1}{\lambda} \log(U)$ the time of next event
    \State Let $i = E$ the time of last event
    \While {$ i \le 1$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $E = \frac{-1}{\lambda} \log(U)$
    \State $i = i + E$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

In the previous the Poisson random variable $X$ is defined as $X = \argmax_n{\sum_{i = 0}^n E_i \le 1}$ with $E_i = = \frac{-1}{\lambda} \log(U_i)$. Therefore $X = \argmax_n{\frac{-1}{\lambda} \sum_{i = 0}^n \log(U_i) \le 1} = \argmax_n{\frac{-1}{\lambda} \log( \Pi_{i = 0}^n U_i ) \le 1}$ and finally $X = \argmin_n \Pi_{i = 0}^n U_i > e^{-\lambda}$. Therefore the third algorithm is described by the following pseudocode.

\begin{algorithm}
  \caption{Generation of a Poisson($\lambda$) with product of uniforms}\label{prod}
  \begin{algorithmic}[1]
    \Procedure{}{}
    \State Let $X = 0$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $i = U$
    \While {$ i \le e^{-\lambda}$}
    \State $X = X + 1$
    \State Let $U$ be a number, generated from a $U[0,1]$ distribution
    \State Let $i = Ui$
    \EndWhile
    \State return $X$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

All three methods have a computational complexity which is proportional to $\lambda$, as it can be seen in Figure~\ref{fig:poi_iter}. However the implementation of the third algorithm is much more expensive. This can be seen in

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.8\textwidth]{images/poi_iter}
  \caption{Iterations required to generate $N = 1000$ Poisson random variables with the three methods}
  \label{fig:poi_iter}
\end{figure}

\section{Exercise 4}
Let the first linear congruential generator (LCG1) have $a = 18, m = 101, c = 0$ and the second (LCG2) $a = 2$ and the same $m, c$. They are both full period. Indeed by generating a sequence of $m-1$ samples there are no repeated samples for both of them. \\
Figures~\ref{fig:lcg1}and~\ref{fig:lcg2} contain the lag plot for lag 1 of these generators. It can seen that the samples are strongly correlated because of the bad choices of the LCGs parameters. Actually, up to the wrap around, for any choice of $x_0$, the sample $n+1$ is $a$ times the sample $n$. Moreover, since the number of possible values is small, the randomness of the sequence is limited.

\begin{figure}
  \centering
  \includegraphics[width = 0.6\textwidth]{images/hw2_4_lcg1}
  \caption{Lag plot for LCG1 at lag 1}
  \label{fig:lcg1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width = 0.6\textwidth]{images/hw2_4_lcg2}
  \caption{Lag plot for LCG2 at lag 1}
  \label{fig:lcg2}
\end{figure}

\section{Exercise 5}


\begin{thebibliography}{10}

\bibitem{leb}
Y. Le Boudec, Performance Evaluation of Computer and Communications Systems, EPFL, 2015

\bibitem{pk}
M. Pinsky, S. Karlin, An Introduction to Stochastic Modeling, $4^{th}$ edition, Elsevier, 2011


\end{thebibliography}

\end{document}
